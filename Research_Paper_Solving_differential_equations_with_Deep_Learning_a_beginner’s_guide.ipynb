{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMuCueiIJ9BOk1Gr9muYrP5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/datamino/PINNs-Research/blob/main/Research_Paper_Solving_differential_equations_with_Deep_Learning_a_beginner%E2%80%99s_guide.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Solving differential equations with Deep Learning: a beginner‚Äôs guide**"
      ],
      "metadata": {
        "id": "AJq-de1GlrxJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "# **Step # 1: Introduction**\n",
        "\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "t4zarjHUO10A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## **üéØ a.Overview**\n",
        "## **üîç Big Picture of the Paper**  \n",
        "\n",
        "This research paper is a **beginner-friendly guide to PINNs (Physics-Informed Neural Networks)**. It introduces the **concepts, methods, and applications** of PINNs for solving **differential equations**. The focus is on making it accessible for **students and researchers** who may not be experts in deep learning or physics.  \n",
        "\n",
        "Here‚Äôs a **step-by-step breakdown** of what this paper is trying to teach:  \n",
        "\n",
        "### **1Ô∏è‚É£ Why Do We Need PINNs?**\n",
        "- Many scientific problems rely on **differential equations** (ODEs and PDEs).  \n",
        "- Traditional methods (e.g., finite element methods) **struggle with complex problems** and require a lot of computational power.  \n",
        "- Neural networks can **approximate any function**, making them powerful tools for solving differential equations.  \n",
        "- **PINNs combine physics with deep learning** to solve differential equations **without needing labeled data**.\n",
        "\n",
        "### **2Ô∏è‚É£ What Are PINNs?**\n",
        "- PINNs are **standard neural networks** with a key difference:  \n",
        "  - Instead of training on labeled data, they **embed differential equations** into the loss function.  \n",
        "  - The loss function enforces **both the differential equation and boundary/initial conditions**.  \n",
        "- This allows PINNs to find solutions to equations **without requiring explicit data points**.\n",
        "\n",
        "### **3Ô∏è‚É£ How Do PINNs Work?**\n",
        "- A neural network is used to **approximate the unknown function** (solution of the equation).  \n",
        "- **Automatic differentiation (AD)** is used to compute derivatives inside the network.  \n",
        "- The network minimizes a **loss function** that ensures:  \n",
        "  1. The differential equation is satisfied at multiple points.  \n",
        "  2. The boundary/initial conditions are met.  \n",
        "\n",
        "### **4Ô∏è‚É£ What Problems Does This Paper Solve?**\n",
        "- The paper demonstrates **three types of differential equations** solved using PINNs:\n",
        "  1. **First-Order ODEs** (Exponential Decay)  \n",
        "  2. **Second-Order ODEs** (Harmonic Oscillator)  \n",
        "  3. **Second-Order Nonlinear ODEs** (Solitons in the KdV Equation)  \n",
        "- For each case, the paper shows:  \n",
        "  - The **mathematical formulation** of the equation.  \n",
        "  - How to implement it using PINNs.  \n",
        "  - How well PINNs approximate the exact solution.  \n",
        "\n",
        "### **5Ô∏è‚É£ Key Takeaways**\n",
        "- PINNs **remove the need for mesh-based numerical methods** (like finite element methods).  \n",
        "- They provide a **flexible and efficient way to solve differential equations**.  \n",
        "- PINNs work well for problems in **fluids, quantum mechanics, and photonics**.  \n",
        "- The authors provide **Python code** so students can experiment with PINNs easily.\n",
        "\n",
        "---\n",
        "\n",
        "## **üìÑ Summary of the Paper**  \n",
        "\n",
        "- **Title:** *Solving Differential Equations with Deep Learning: A Beginner‚Äôs Guide*  \n",
        "- **Authors:** Luis Medrano Navarro, Luis Martin Moreno, Sergio G. Rodrigo  \n",
        "- **Affiliation:** Universidad de Zaragoza, Spain  \n",
        "- **Main Goal:** Introduce **PINNs in a simple way** for physics and engineering students.  \n",
        "- **Key Sections:**  \n",
        "  1. **Introduction** ‚Äì Importance of AI in scientific computing.  \n",
        "  2. **Basics of Neural Networks** ‚Äì How NNs work, including activation functions, loss functions, and backpropagation.  \n",
        "  3. **PINNs Explanation** ‚Äì How they incorporate physics into neural networks.  \n",
        "  4. **Solving ODEs with PINNs** ‚Äì Example implementations for different equations.  \n",
        "  5. **Comparison with Classical Methods** ‚Äì PINNs vs. traditional solvers.  \n",
        "  6. **Conclusion** ‚Äì How PINNs can revolutionize physics education.  \n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "x7hLwygPkklY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "# **Step # 2: Deep Reading (Line-by-Line Understanding)**\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "81H6E4gluvAa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **üéØ 1.Introduction**\n"
      ],
      "metadata": {
        "id": "M95ip6qx6kZv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **üìå a. Introduction**  \n",
        "\n",
        "### **üîπ Overview of AI in Solving Differential Equations**  \n",
        "Artificial Intelligence (AI) has transformed many scientific and engineering fields, including **solving differential equations**, which are essential in modeling physical systems.  \n",
        "\n",
        "- Differential equations are widely used in **physics, engineering, and applied mathematics** to describe **heat transfer, fluid dynamics, quantum mechanics, and many other natural processes**.  \n",
        "- Traditionally, solving these equations required **numerical methods** like **Finite Element Methods (FEM), Finite Difference Methods (FDM), and Spectral Methods**.  \n",
        "- However, **these methods have limitations**, such as **high computational cost, sensitivity to meshing, and difficulties in handling complex geometries**.  \n",
        "\n",
        "To overcome these limitations, researchers explored **Neural Networks (NNs) as function approximators** to solve differential equations **without requiring traditional numerical solvers**.  \n",
        "\n",
        "---\n",
        "\n",
        "## **üîπ Early Approaches: Trial Function Method (1999 Approach)**  \n",
        "Before modern Physics-Informed Neural Networks (PINNs), early AI-based methods in the **1990s** attempted to solve differential equations using **trial functions**.  \n",
        "\n",
        "### **What is a Trial Function?**  \n",
        "A **trial function** is a function that is **constructed to automatically satisfy the boundary conditions** of a differential equation. Instead of directly training a neural network to predict $( y(x))$, researchers designed a function of the form:\n",
        "\n",
        "$$\n",
        "y_{\\text{trial}}(x) = A(x) + B(x) N(x, W)\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $( A(x))$ **strictly satisfies the boundary conditions**.  \n",
        "- $( B(x) )$ ensures that the **neural network output does not interfere with boundary conditions**.  \n",
        "- $( N(x, W)$) is a **neural network with trainable weights \\( W \\)** that learns the unknown part of the solution.  \n",
        "\n",
        "#### **How Did It Work? (Step-by-Step Example)**\n",
        "Let‚Äôs consider solving the **ODE**:\n",
        "\n",
        "$$\n",
        "y'(x) + y(x) = 0, \\quad y(0) = 1\n",
        "$$\n",
        "\n",
        "1. **Choosing a function that satisfies the boundary condition**  \n",
        "   - A simple choice: $( A(x) = 1)$ because it satisfies $( A(0) = 1 )$.  \n",
        "2. **Choosing a function $( B(x) )$ that vanishes at the boundary**  \n",
        "   - A good choice is $( B(x) = x )$, because $( B(0) = 0 )$, ensuring the NN won‚Äôt interfere with boundary conditions.  \n",
        "3. **Defining the trial function**  \n",
        "   $$\n",
        "   y_{\\text{trial}}(x) = 1 + x N(x, W)\n",
        "   $$\n",
        "   - Here, $( N(x, W))$ is a neural network that learns the unknown part of the solution.  \n",
        "\n",
        "The **neural network was trained** to satisfy the differential equation, ensuring that the final solution met both the **ODE and the boundary conditions**.\n",
        "\n",
        "#### **Limitations of the Trial Function Approach**  \n",
        "‚ùå **Manual selection of $( A(x))$ and $( B(x))$** makes it difficult to generalize to complex equations.  \n",
        "‚ùå **Not suitable for partial differential equations (PDEs)**.  \n",
        "‚ùå **Difficult to apply to high-dimensional problems**.  \n",
        "‚ùå **Does not generalize well to different types of differential equations**.  \n",
        "\n",
        "Because of these limitations, researchers moved toward **more flexible, neural-network-based methods**, leading to the development of modern **Neural Operator Networks**.\n",
        "\n",
        "---\n",
        "\n",
        "## **üîπ Modern Approaches: Fourier Neural Operator Networks (FNOs) & DeepONet**  \n",
        "Since trial functions were **too restrictive**, researchers developed **Fourier Neural Operator Networks (FNOs) and Deep Operator Networks (DeepONet)**, which generalize neural networks to **learn entire function mappings instead of just solving equations at discrete points**.\n",
        "\n",
        "### **1Ô∏è‚É£ Fourier Neural Operator Networks (FNOs)**  \n",
        "**Fourier Neural Operator Networks (FNOs)** leverage **Fourier Transforms** to efficiently solve differential equations by capturing **global dependencies** instead of just local information.\n",
        "\n",
        "#### **How FNOs Work (Step-by-Step)**\n",
        "1. **Convert the Input Function into Fourier Space**  \n",
        "   - Use the **Fast Fourier Transform (FFT)** to transform the function into frequency space.  \n",
        "2. **Apply a Neural Network in Fourier Space**  \n",
        "   - The network learns relationships between different frequencies in the problem.  \n",
        "3. **Convert Back to Original Space**  \n",
        "   - Use the **Inverse Fourier Transform (IFFT)** to return to physical space.  \n",
        "\n",
        "#### **Why FNOs Are Powerful**  \n",
        "‚úÖ **Faster than traditional PDE solvers**.  \n",
        "‚úÖ **Captures long-range dependencies** (important for physics problems).  \n",
        "‚úÖ **Generalizes across different resolutions** (unlike CNNs).  \n",
        "\n",
        "---\n",
        "\n",
        "### **2Ô∏è‚É£ Deep Operator Networks (DeepONet)**\n",
        "**DeepONet** extends neural networks to learn **operators**, meaning it maps **entire functions to other functions**.\n",
        "\n",
        "#### **How Does DeepONet Work?**\n",
        "- It consists of **two networks**:  \n",
        "  1. **Branch Network** ‚Äì Encodes the input function $( f(x))$.  \n",
        "  2. **Trunk Network** ‚Äì Processes specific query points to predict $( G(f)(x))$.  \n",
        "- The final prediction is a **weighted sum** of the outputs of both networks.\n",
        "\n",
        "#### **Why DeepONet is Powerful?**  \n",
        "‚úÖ **Learns function-to-function mappings** (not just point-wise approximations).  \n",
        "‚úÖ **Generalizes well across different PDEs and scientific problems**.  \n",
        "‚úÖ **Reduces the need for labeled data** by learning the structure of the solution.  \n",
        "\n",
        "---\n",
        "\n",
        "## **üîπ Physics-Informed Neural Networks (PINNs) ‚Äì A New Paradigm**  \n",
        "To **fully integrate physics into deep learning**, researchers introduced **Physics-Informed Neural Networks (PINNs)**.\n",
        "\n",
        "### **What Are PINNs?**  \n",
        "PINNs use **deep learning to solve differential equations** by embedding physics laws **directly into the loss function**.\n",
        "\n",
        "Instead of learning from **labeled data**, PINNs learn solutions by **minimizing the residual of the differential equation**.  \n",
        "\n",
        "$$\n",
        "L(\\theta) = \\omega_D L_D(\\theta) + \\omega_B L_B(\\theta)\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $( L_D(\\theta))$ ensures the **differential equation is satisfied**.  \n",
        "- $( L_B(\\theta))$ ensures **boundary/initial conditions are met**.  \n",
        "- $( \\omega_D)$ and $( \\omega_B)$ balance the two losses.  \n",
        "\n",
        "### **Why Are PINNs Useful?**  \n",
        "‚úÖ **Do not require labeled data**‚Äîthey learn directly from equations.  \n",
        "‚úÖ **Can solve both ODEs and PDEs**.  \n",
        "‚úÖ **Work in high-dimensional spaces**, unlike traditional solvers.  \n",
        "‚úÖ **Used in physics, fluid dynamics, quantum mechanics, and more**.  \n",
        "\n",
        "---\n",
        "\n",
        "## **üìå Final Summary (Introduction Section Overview)**  \n",
        "1Ô∏è‚É£ **Early Approaches (Trial Functions - 1999)**  \n",
        "   - Used predefined **trial functions** to ensure boundary conditions.  \n",
        "   - **Limitations**: Not generalizable, hard to apply to PDEs.  \n",
        "\n",
        "2Ô∏è‚É£ **Modern Methods (Neural Operators)**\n",
        "   - **Fourier Neural Operator Networks (FNOs)** ‚Äì Uses **Fourier Transforms** to capture **global dependencies** in PDEs.  \n",
        "   - **DeepONet** ‚Äì Uses a **two-branch network** to learn **function-to-function mappings**.  \n",
        "\n",
        "3Ô∏è‚É£ **PINNs (Physics-Informed Neural Networks)**\n",
        "   - **Combine physics and deep learning** by embedding differential equations into the **loss function**.  \n",
        "   - **Solve ODEs/PDEs without labeled data**.  \n",
        "\n",
        "### **Key Takeaways**  \n",
        "‚úÖ **Traditional numerical methods struggle with complex differential equations.**  \n",
        "‚úÖ **Trial functions were an early approach but had major limitations.**  \n",
        "‚úÖ **Modern methods (FNOs & DeepONet) allow neural networks to learn operators.**  \n",
        "‚úÖ **PINNs solve differential equations without requiring labeled datasets.**  \n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Xo_kJ4HGlobM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## **üìå b. More Explaination**\n"
      ],
      "metadata": {
        "id": "QBvm5s5KwuiE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. What Was the 1999 Approach? (Trial Function)**  \n",
        "\n",
        "- In 1999, researchers like **Lagaris et al.** proposed an early way to solve differential equations using **Artificial Neural Networks (ANNs)**.  \n",
        "- Their idea was to **define a trial function** that **automatically satisfies boundary conditions** and then train the network to satisfy the differential equation.  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå What Is a Trial Function?**  \n",
        "A **trial function** is a function that is **constructed in a way that it already satisfies the boundary conditions** of the differential equation.  \n",
        "\n",
        "Let‚Äôs say we want to solve this **ordinary differential equation (ODE)**:  \n",
        "$$\n",
        "y'(x) + y(x) = 0, \\quad \\text{with boundary condition} \\quad y(0) = 1$$\n",
        "Instead of directly training a neural network to predict $( y(x))$, the 1999 approach used a function like this:  \n",
        "\n",
        "$$\n",
        "y_{\\text{trial}}(x) = A(x) + B(x) N(x, W)\n",
        "$$\n",
        "\n",
        "where:  \n",
        "- $( A(x))$ is a function that **strictly satisfies the boundary condition**.  \n",
        "- $( B(x))$ is a function that **ensures the neural network output does not interfere with boundary conditions**.  \n",
        "- $( N(x, W))$ is the **output of a neural network** with trainable weights $( W)$, used to approximate the unknown part of the solution.  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Why Did They Use a Trial Function?**  \n",
        "The biggest problem when training a neural network to solve differential equations is ensuring that the **solution satisfies boundary conditions**.  \n",
        "- If the NN is trained directly, it may not naturally satisfy the boundary conditions.  \n",
        "- Instead, **by embedding boundary conditions into the trial function**, they **forced** the solution to always respect the boundary conditions.  \n",
        "- The only thing left for the NN to learn was the **differential equation itself**.\n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Example of a Trial Function (Step-by-Step)**  \n",
        "Let‚Äôs take a simple ODE with boundary conditions:  \n",
        "$\n",
        "y'(x) + y(x) = 0, \\quad y(0) = 1\n",
        "$\n",
        "1. **Step 1: Choose a function that satisfies the boundary condition**  \n",
        "   - A simple choice: $( A(x) = 1)$ because it satisfies $( A(0) = 1)$.  \n",
        "\n",
        "2. **Step 2: Choose a function $( B(x))$ that vanishes at the boundary**  \n",
        "   - A good choice is $( B(x) = x)$, because $( B(0) = 0)$, ensuring that the NN won‚Äôt interfere with the boundary condition.  \n",
        "\n",
        "3. **Step 3: Define the trial function**  \n",
        "   $$\n",
        "   y_{\\text{trial}}(x) = A(x) + B(x) N(x, W) = 1 + x N(x, W)\n",
        "   $$\n",
        "   - Here, $( N(x, W))$ is a **neural network with trainable weights $( W) $**.  \n",
        "\n",
        "4. **Step 4: Train the Neural Network**  \n",
        "   - Instead of training the NN to directly predict $( y(x))$, it learns $( N(x, W)) $.  \n",
        "   - The network is optimized so that $( y_{\\text{trial}}(x))$ **satisfies the differential equation**.  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå Why Was This Approach Limited?**\n",
        "1. **Choosing $( A(x))$ and $( B(x))$ requires manual effort** ‚Üí Different problems require different trial functions.  \n",
        "2. **Works only for simple boundary conditions** ‚Üí For complex problems (like PDEs), defining a good trial function is difficult.  \n",
        "3. **Generalization is weak** ‚Üí The network does not learn a **pure function approximation**, but rather a constrained version of it.  \n",
        "\n",
        "Because of these issues, researchers later **removed the need for a trial function** and instead incorporated the differential equation into the **loss function**, leading to **modern PINNs**.\n",
        "\n",
        "---\n",
        "\n",
        "### **üîπ Summary of the 1999 Approach:**\n",
        "‚úÖ **Used a \"trial function\" to ensure boundary conditions were always satisfied.**  \n",
        "‚úÖ **Neural network learned only the unknown part of the solution.**  \n",
        "‚úÖ **Required manually choosing \\( A(x) \\) and \\( B(x) \\), making it limited.**  \n",
        "‚úÖ **Modern PINNs no longer use trial functions and instead enforce equations through the loss function.**  \n"
      ],
      "metadata": {
        "id": "L7JeikJUypzC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Fourier Neural Operator Networks (FNO)**  \n",
        "\n",
        "\n",
        "## **1Ô∏è‚É£ What is the Fourier Series?**  \n",
        "The **Fourier Series** is a mathematical method that allows us to represent **any periodic function** as an **infinite sum of sines and cosines**.  \n",
        "\n",
        "### **üîπ Why Do We Need Fourier Series?**  \n",
        "Some functions (like square waves, repeating patterns, or real-world signals) **cannot be easily written as polynomials or other simple functions**. Instead, we can express them as an infinite sum of **sinusoidal functions (sines & cosines)**.  \n",
        "\n",
        "### **üîπ Fourier Series Formula**  \n",
        "For a function $f(x)$ that is periodic over $[-L, L]$, its Fourier Series representation is:  \n",
        "\n",
        "$$  \n",
        "f(x) = a_0 + \\sum_{n=1}^{\\infty} \\left( a_n \\cos \\frac{n\\pi x}{L} + b_n \\sin \\frac{n\\pi x}{L} \\right)  \n",
        "$$  \n",
        "\n",
        "where:  \n",
        "- $a_0$ = Constant term (DC component).  \n",
        "- $a_n$ and $b_n$ are **Fourier coefficients** that determine how much of each sine/cosine wave is needed.  \n",
        "- $\\cos$ and $\\sin$ terms represent different **frequencies** in the function.  \n",
        "\n",
        "### **üîπ Example: Approximating a Square Wave**  \n",
        "A **square wave** is a function that alternates between $+1$ and $-1$. A square wave is **not smooth**, but we can approximate it using **only sines**:  \n",
        "\n",
        "$$  \n",
        "f(x) = \\frac{4}{\\pi} \\sum_{n=1,3,5,\\dots}^{\\infty} \\frac{1}{n} \\sin \\left( n x \\right)  \n",
        "$$  \n",
        "\n",
        "The more terms we add, the closer we get to the actual **square wave**.  \n",
        "\n",
        "üëâ **Key Idea:** The Fourier Series helps us express **complex functions as a combination of simple waveforms** (sines & cosines).  \n",
        "\n",
        "---\n",
        "\n",
        "## **2Ô∏è‚É£ What is a Fourier Neural Operator (FNO)?**  \n",
        "Now that we understand Fourier Series, let‚Äôs see how it applies to **Fourier Neural Operator Networks (FNOs)**.  \n",
        "\n",
        "### **üîπ Why Was FNO Introduced?**  \n",
        "- Traditional neural networks struggle with **solving partial differential equations (PDEs)** because they rely on **local information** (point-wise data).  \n",
        "- PDEs describe **global relationships**, meaning that information at one point influences the entire domain.  \n",
        "- **FNOs use the Fourier Transform to capture global relationships efficiently**.  \n",
        "\n",
        "### **üîπ What is a Neural Operator?**  \n",
        "Before FNOs, researchers developed **Neural Operators**, which generalize neural networks to learn **functions instead of just mappings between inputs and outputs**.  \n",
        "\n",
        "**Traditional NN:**  \n",
        "$$  \n",
        "f: x \\to y  \n",
        "$$  \n",
        "(Takes input $x$, outputs $y$).  \n",
        "\n",
        "**Neural Operator:**  \n",
        "$$  \n",
        "\\mathcal{F}: f(x) \\to g(x)  \n",
        "$$  \n",
        "(Takes an entire function $f(x)$ as input, outputs another function $g(x)$).  \n",
        "\n",
        "This means **instead of just learning data points, Neural Operators learn continuous functions**.  \n",
        "\n",
        "---\n",
        "\n",
        "## **3Ô∏è‚É£ How Do Fourier Neural Operator Networks (FNOs) Work?**  \n",
        "FNOs solve PDEs by using the **Fourier Transform** inside the neural network.  \n",
        "\n",
        "### **üîπ Step-by-Step Process of FNO**  \n",
        "1. **Transform Input into Fourier Space**  \n",
        "   - Instead of working with data directly, FNO applies the **Fast Fourier Transform (FFT)** to convert input into the frequency domain.  \n",
        "2. **Apply a Neural Network in Fourier Space**  \n",
        "   - In the frequency domain, FNO learns how different frequencies interact.  \n",
        "3. **Transform Back to Original Space**  \n",
        "   - Uses the **Inverse Fourier Transform (IFFT)** to get the final output.  \n",
        "\n",
        "### **üîπ Key Formula Behind FNO**  \n",
        "Let‚Äôs say we want to solve a PDE where we predict a function $u(x)$ given input $f(x)$.  \n",
        "FNO learns an operator **$\\mathcal{F}$** such that:  \n",
        "\n",
        "$$  \n",
        "\\mathcal{F}(f(x)) = u(x)  \n",
        "$$  \n",
        "\n",
        "It does this using Fourier transforms:  \n",
        "\n",
        "$$  \n",
        "\\mathcal{F} = W_1 + \\mathcal{F}^{-1} \\sigma ( W_2 \\mathcal{F} f )  \n",
        "$$  \n",
        "\n",
        "where:  \n",
        "- $\\mathcal{F}$ is the **Fourier Transform**.  \n",
        "- $W_1, W_2$ are **trainable weights** in Fourier space.  \n",
        "- $\\mathcal{F}^{-1}$ is the **Inverse Fourier Transform**.  \n",
        "- $\\sigma$ is a **non-linearity (like ReLU)**.  \n",
        "\n",
        "### **üîπ What Makes FNO Special?**  \n",
        "‚úÖ **Captures global information** ‚Üí Unlike CNNs, which only use local neighborhoods, FNOs capture **long-range dependencies**.  \n",
        "‚úÖ **Reduces computational cost** ‚Üí Working in Fourier space makes it much **faster than traditional PDE solvers**.  \n",
        "‚úÖ **Works on any resolution** ‚Üí Unlike CNNs, which require fixed grid sizes, FNOs **generalize well to different resolutions**.  \n",
        "\n",
        "---\n",
        "\n",
        "## **4Ô∏è‚É£ Example: Using FNO to Solve a PDE**  \n",
        "Let‚Äôs say we want to solve the **Navier-Stokes equation** (which models fluid flow).  \n",
        "\n",
        "### **Step 1: Define the PDE**  \n",
        "The **Navier-Stokes equation** in 2D is:  \n",
        "\n",
        "$$  \n",
        "\\frac{\\partial u}{\\partial t} + u \\cdot \\nabla u = -\\nabla p + \\nu \\nabla^2 u  \n",
        "$$  \n",
        "\n",
        "where:  \n",
        "- $u$ is velocity,  \n",
        "- $p$ is pressure,  \n",
        "- $\\nu$ is viscosity.  \n",
        "\n",
        "### **Step 2: Convert to Fourier Space**  \n",
        "We apply the **Fourier Transform** to the equation:  \n",
        "\n",
        "$$  \n",
        "\\mathcal{F} \\left( \\frac{\\partial u}{\\partial t} \\right) + \\mathcal{F} ( u \\cdot \\nabla u ) = -\\mathcal{F} (\\nabla p) + \\nu \\mathcal{F} (\\nabla^2 u)  \n",
        "$$  \n",
        "\n",
        "### **Step 3: Train the Neural Network**  \n",
        "- Instead of solving this equation numerically, we **train FNO to learn the mapping from initial conditions to future velocity fields**.  \n",
        "- FNO takes the input **in Fourier space**, processes it using a neural network, and **outputs the solution**.  \n",
        "\n",
        "### **Step 4: Transform Back to Original Space**  \n",
        "Finally, we apply the **Inverse Fourier Transform** to get the solution in real space.  \n",
        "\n",
        "---\n",
        "\n",
        "## **5Ô∏è‚É£ Comparison: FNO vs. Other Methods**  \n",
        "| **Method**             | **Computational Efficiency** | **Captures Long-Range Interactions** | **Generalization to Different Resolutions** |  \n",
        "|------------------------|----------------------------|------------------------------------|--------------------------------|  \n",
        "| Finite Element Method  | ‚ùå Slow                     | ‚úÖ Yes                             | ‚ùå No                           |  \n",
        "| CNNs for PDEs         | ‚úÖ Fast                     | ‚ùå No (only local)                 | ‚ùå No                           |  \n",
        "| FNO                   | ‚úÖ Fast                     | ‚úÖ Yes                             | ‚úÖ Yes                           |  \n",
        "\n",
        "üëâ **Key Takeaway:** **FNOs combine the speed of deep learning with the mathematical power of Fourier analysis.**  \n",
        "\n",
        "---\n",
        "\n",
        "## **6Ô∏è‚É£ Summary**  \n",
        "‚úÖ **Fourier Series** expresses functions as **sines & cosines**.  \n",
        "‚úÖ **Fourier Neural Operators (FNOs)** use **Fourier Transforms inside neural networks** to solve PDEs.  \n",
        "‚úÖ **FNOs capture global interactions efficiently**, unlike CNNs, which only use local information.  \n",
        "‚úÖ **FNOs generalize across resolutions**, making them powerful for physics-based problems.  \n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "yI75N6xb1ihn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. Deep Operator Networks (DeepONet)**  \n",
        "\n",
        "DeepONet (Deep Operator Networks) is another modern deep learning approach for solving **differential equations and operator learning**. Before we dive into DeepONet, let‚Äôs first break down what an **operator** is and how it connects to the **Approximation Theorem** mentioned in the paper.  \n",
        "\n",
        "---\n",
        "\n",
        "## **1Ô∏è‚É£ What is an Operator?**  \n",
        "In **mathematics and physics**, an **operator** is a function that **maps one function to another function**.  \n",
        "\n",
        "For example, in calculus:  \n",
        "- The **differentiation operator** $\\mathcal{D}$ maps a function $f(x)$ to its derivative $f'(x)$.  \n",
        "- The **integral operator** $\\mathcal{I}$ maps $f(x)$ to its integral $\\int f(x) dx$.  \n",
        "\n",
        "Operators are **very general** and appear in **many real-world problems**, especially in physics, engineering, and fluid dynamics.  \n",
        "\n",
        "---\n",
        "\n",
        "## **2Ô∏è‚É£ What is the Approximation Theorem & How Does It Inspire DeepONet?**  \n",
        "The **Universal Approximation Theorem** states that:  \n",
        "> A neural network with **at least one hidden layer** can approximate **any continuous function** to any desired accuracy.  \n",
        "\n",
        "This theorem applies to standard deep learning models, where we approximate **functions** like:  \n",
        "$$  \n",
        "f(x) \\approx \\text{NN}(x)  \n",
        "$$  \n",
        "where NN is a neural network.  \n",
        "\n",
        "However, in real-world physics and engineering problems, we often don‚Äôt just approximate **functions**‚Äîwe approximate **operators**, meaning mappings between entire functions.  \n",
        "\n",
        "üîπ **DeepONet extends the Universal Approximation Theorem to operators.**  \n",
        "It states that:  \n",
        "> A neural network can **learn mappings between functions**, not just mappings from inputs to outputs.  \n",
        "\n",
        "This means DeepONet is **not just learning numbers**‚Äîit is **learning how to transform entire functions**, making it extremely powerful for solving differential equations.  \n",
        "\n",
        "---\n",
        "\n",
        "## **3Ô∏è‚É£ What is DeepONet & How Does It Work?**  \n",
        "DeepONet is a **neural network architecture designed to learn operators**. Instead of learning **function values**, DeepONet learns a mapping between **entire functions**.  \n",
        "\n",
        "### **üîπ Why Was DeepONet Introduced?**  \n",
        "- Traditional neural networks learn to approximate **single function values**.  \n",
        "- In physics and PDE problems, we often need to approximate **how one function transforms into another**.  \n",
        "- DeepONet is designed specifically for this purpose: **learning operators**.  \n",
        "\n",
        "### **üîπ DeepONet Architecture (Two-Branch Network)**  \n",
        "DeepONet consists of **two neural networks working together**:  \n",
        "\n",
        "1Ô∏è‚É£ **Branch Network (Encodes the Input Function)**  \n",
        "   - Takes a function $f(x)$ and encodes it into a finite-dimensional representation.  \n",
        "   - Instead of feeding just one value $x$, it takes **multiple points** from $f(x)$.  \n",
        "   - **Example:** If $f(x)$ is a temperature distribution over space, the Branch Network captures the **entire function**.  \n",
        "\n",
        "2Ô∏è‚É£ **Trunk Network (Processes Query Points)**  \n",
        "   - Takes specific evaluation points $x$ and learns how to predict the output function.  \n",
        "   - This allows the model to make predictions at any new location $x$.  \n",
        "\n",
        "### **üîπ DeepONet Formula**  \n",
        "DeepONet learns an **operator $G(f)$** that maps functions $f(x)$ to another function $g(x)$.  \n",
        "\n",
        "Mathematically, DeepONet approximates:  \n",
        "\n",
        "$$  \n",
        "G(f)(x) \\approx \\sum_{i=1}^{p} b_i(f) t_i(x)  \n",
        "$$  \n",
        "\n",
        "where:  \n",
        "- $b_i(f)$ are outputs from the **Branch Network** (learned function embeddings).  \n",
        "- $t_i(x)$ are outputs from the **Trunk Network** (evaluations at specific points).  \n",
        "\n",
        "This means **DeepONet learns a continuous mapping between functions, not just single values**.  \n",
        "\n",
        "---\n",
        "\n",
        "## **4Ô∏è‚É£ Example: Using DeepONet to Solve a PDE**  \n",
        "Let‚Äôs say we want to solve the **Poisson equation**:  \n",
        "\n",
        "$$  \n",
        "\\nabla^2 u(x) = f(x), \\quad x \\in \\Omega  \n",
        "$$  \n",
        "\n",
        "where:  \n",
        "- $u(x)$ is the unknown function.  \n",
        "- $f(x)$ is a given function (forcing term).  \n",
        "- $\\nabla^2$ is the Laplace operator.  \n",
        "\n",
        "### **Step 1: Train DeepONet**  \n",
        "- The **Branch Network** takes $f(x)$ as input and encodes it.  \n",
        "- The **Trunk Network** learns how $u(x)$ depends on $x$.  \n",
        "\n",
        "### **Step 2: Predict the Solution**  \n",
        "- Given a new function $f(x)$, DeepONet can **immediately predict the solution function $u(x)$** without solving the PDE numerically.  \n",
        "\n",
        "---\n",
        "\n",
        "## **5Ô∏è‚É£ What Makes DeepONet Special?**  \n",
        "| **Feature**         | **Traditional Neural Networks** | **DeepONet** |  \n",
        "|--------------------|------------------------------|-------------|  \n",
        "| Learns function values? | ‚úÖ Yes | ‚úÖ Yes |  \n",
        "| Learns function-to-function mappings? | ‚ùå No | ‚úÖ Yes |  \n",
        "| Works for PDEs? | ‚ö†Ô∏è Limited | ‚úÖ Excellent |  \n",
        "| Works with sparse data? | ‚ùå No | ‚úÖ Yes |  \n",
        "| Generalizes across different problems? | ‚ùå No | ‚úÖ Yes |  \n",
        "\n",
        "üîπ **Key Difference:**  \n",
        "DeepONet learns **how an entire function transforms into another function**, making it perfect for PDEs.  \n",
        "\n",
        "---\n",
        "\n",
        "## **6Ô∏è‚É£ Summary**  \n",
        "‚úÖ **Operators** map functions to other functions (e.g., differentiation, integration).  \n",
        "‚úÖ The **Approximation Theorem** states that deep networks can approximate any function-to-function mapping.  \n",
        "‚úÖ **DeepONet** is a special neural network that learns **operators instead of just functions**.  \n",
        "‚úÖ It consists of a **Branch Network (function encoder)** and a **Trunk Network (query-based evaluation)**.  \n",
        "‚úÖ DeepONet is **highly efficient for solving PDEs and physics-based problems**.  \n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "AETXGkqs2E4V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **üéØ 2. Basics of Neural Networks**  "
      ],
      "metadata": {
        "id": "ANyw9a_l2VFR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **üìå a. Basics of Neural Networks**  \n",
        "\n",
        "\n",
        "\n",
        "## **1Ô∏è‚É£ How a Neural Network Mimics a Biological Neuron**  \n",
        "\n",
        "A **biological neuron** in the human brain processes information as follows:  \n",
        "\n",
        "1Ô∏è‚É£ **Dendrites** ‚Üí Collect input signals from other neurons.  \n",
        "2Ô∏è‚É£ **Cell Body (Soma)** ‚Üí Adds up the received signals, weighing them based on importance.  \n",
        "3Ô∏è‚É£ **Axon** ‚Üí If the total signal **exceeds a certain threshold**, the neuron **triggers** and sends a signal to the next neuron.  \n",
        "4Ô∏è‚É£ **Synapse** ‚Üí The output signal is transmitted to other neurons via connections.  \n",
        "\n",
        "üí° **Key Idea**: Neurons act as **threshold-based decision units**‚Äîif the input is strong enough, the neuron fires; otherwise, it remains inactive.  \n",
        "\n",
        "A **mathematical neuron** follows a similar process:  \n",
        "\n",
        "1Ô∏è‚É£ Takes multiple inputs $x_1, x_2, ..., x_n$.  \n",
        "2Ô∏è‚É£ Applies a **weighted sum**:  \n",
        "   $$  \n",
        "   z = \\sum_{i=1}^{n} w_i x_i + b  \n",
        "   $$  \n",
        "3Ô∏è‚É£ Passes it through an **activation function** to introduce non-linearity:  \n",
        "   $$  \n",
        "   y = f(z)  \n",
        "   $$  \n",
        "\n",
        "\n",
        "### **&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Neural Network**\n",
        "\n",
        "  <center>\n",
        "  <img src=\"https://tikz.net/wp-content/uploads/2021/12/neural_networks-003.png\" alt=\"Image loading error. Image of Neural Network is here.\" width=\"300\" > </center>\n",
        "  <br>\n",
        "\n",
        "---\n",
        "\n",
        "## **2Ô∏è‚É£ Activation Functions**  \n",
        "\n",
        "A neural network needs **activation functions** to introduce **non-linearity**. Without them, the network behaves like a **linear function**, limiting its ability to learn complex patterns.  \n",
        "\n",
        "### **üîπ Early Activation Function (Perceptron - 1958)**  \n",
        "\n",
        "The **first neural networks (Perceptrons)** used the **Heaviside Step Function**, which is defined as:  \n",
        "\n",
        "$$  \n",
        "H(z) =  \n",
        "\\begin{cases}  \n",
        "1, & \\text{if } z \\geq 0 \\\\  \n",
        "0, & \\text{if } z < 0  \n",
        "\\end{cases}  \n",
        "$$  \n",
        "\n",
        "üí° **Why was it abandoned?**  \n",
        "‚ùå It is **not differentiable** at $z=0$, which makes training difficult using gradient descent.  \n",
        "‚ùå It only outputs **0 or 1**, making learning slow and inefficient.  \n",
        "\n",
        "### **üîπ Modern Activation Functions**  \n",
        "\n",
        "- **Sigmoid Function** (Used in early NNs):  \n",
        "  $$  \n",
        "  \\sigma(z) = \\frac{1}{1 + e^{-z}}  \n",
        "  $$  \n",
        "  ‚úÖ Squashes values between **0 and 1**.  \n",
        "  ‚ùå Suffers from **vanishing gradient** problems in deep networks.  \n",
        "\n",
        "- **ReLU (Rectified Linear Unit)** (Most commonly used today):  \n",
        "  $$  \n",
        "  f(z) = \\max(0, z)  \n",
        "  $$  \n",
        "  ‚úÖ Solves the vanishing gradient issue.  \n",
        "  ‚úÖ Helps deep networks train faster.  \n",
        "\n",
        "---\n",
        "\n",
        "## **3Ô∏è‚É£ Structure of a Deep Neural Network (DNN)**  \n",
        "\n",
        "A standard **Deep Neural Network (DNN)** consists of multiple layers:  \n",
        "\n",
        "1Ô∏è‚É£ **Input Layer** ‚Üí Takes raw input values (e.g., $x$ in an equation).  \n",
        "2Ô∏è‚É£ **Hidden Layers** ‚Üí Apply transformations using weighted sums & activation functions.  \n",
        "3Ô∏è‚É£ **Output Layer** ‚Üí Produces the final predicted value $y_{\\text{NN}}(x)$.  \n",
        "\n",
        "Each hidden layer refines the input representation, making it useful for solving **complex problems**.  \n",
        "\n",
        "---\n",
        "\n",
        "## **4Ô∏è‚É£ Loss Function (How Neural Networks Learn)**  \n",
        "\n",
        "To improve predictions, neural networks measure how **far off** they are from the true values using a **loss function**.  \n",
        "\n",
        "A commonly used loss function is **Mean Squared Error (MSE)**:  \n",
        "\n",
        "$$  \n",
        "MSE = \\frac{1}{n} \\sum (y_{\\text{true}} - y_{\\text{NN}})^2  \n",
        "$$  \n",
        "\n",
        "‚úÖ **Goal:** Minimize this loss during training so the predicted values get closer to the actual values.  \n",
        "\n",
        "---\n",
        "\n",
        "## **5Ô∏è‚É£ Backpropagation & Gradient Descent**  \n",
        "\n",
        "Neural networks **adjust their weights** using **Gradient Descent**, which is an optimization method that minimizes the loss function.  \n",
        "\n",
        "1Ô∏è‚É£ **Compute Gradients** using **Automatic Differentiation (AD)**.  \n",
        "2Ô∏è‚É£ **Update Weights** using the **Gradient Descent Rule**:  \n",
        "   $$  \n",
        "   w(t+1) = w(t) - \\eta \\frac{\\partial L}{\\partial w}  \n",
        "   $$  \n",
        "   - **$\\eta$** (learning rate) controls how much the weights change.  \n",
        "\n",
        "---\n",
        "\n",
        "## **6Ô∏è‚É£ Automatic Differentiation (AD) & Other Gradient Techniques**  \n",
        "\n",
        "### **üîπ What is Automatic Differentiation (AD)?**  \n",
        "\n",
        "AD is a method used to **efficiently compute derivatives** for training neural networks.  \n",
        "\n",
        "There are **three main ways** to compute derivatives:  \n",
        "\n",
        "1Ô∏è‚É£ **Symbolic Differentiation** ‚Üí Uses algebraic rules to compute exact derivatives.  \n",
        "   ‚ùå **Problem:** Expressions become too large (expression explosion).  \n",
        "\n",
        "2Ô∏è‚É£ **Numerical Differentiation** ‚Üí Uses small step sizes ($h$) to approximate derivatives.  \n",
        "   ‚ùå **Problem:** Leads to rounding errors.  \n",
        "\n",
        "3Ô∏è‚É£ **Automatic Differentiation (AD)** ‚Üí **Most efficient method** for deep learning.  \n",
        "\n",
        "üí° **Key Idea**: AD breaks down computations into elementary operations and applies the **chain rule** automatically.  \n",
        "\n",
        "---\n",
        "\n",
        "## **7Ô∏è‚É£ Assumptions in Backpropagation**  \n",
        "\n",
        "Backpropagation **relies on two key assumptions** about the loss function **$L$**:  \n",
        "\n",
        "1Ô∏è‚É£ **Loss can be expressed as an average over individual training examples:**  \n",
        "   $$  \n",
        "   L = \\frac{1}{n} \\sum L_i  \n",
        "   $$  \n",
        "   - This means the total loss is simply the **average loss** of each training example.  \n",
        "   - **Why?** It makes gradient computation **scalable** (stochastic gradient descent can update weights using mini-batches).  \n",
        "\n",
        "2Ô∏è‚É£ **Loss can be expressed as a function of network outputs:**  \n",
        "   $$  \n",
        "   L = f(y_{\\text{NN}})  \n",
        "   $$  \n",
        "   - The loss function depends **only on the predicted output** of the neural network.  \n",
        "   - **Why?** This allows us to **apply the chain rule** efficiently during backpropagation.  \n",
        "\n",
        "‚úÖ These assumptions **simplify** training and make gradient-based optimization feasible.  \n",
        "\n",
        "---\n",
        "\n",
        "## **üìå Key Takeaways**  \n",
        "\n",
        "‚úÖ **Neural Networks are function approximators**.  \n",
        "‚úÖ **They learn by minimizing loss functions using gradient descent**.  \n",
        "‚úÖ **Activation functions introduce non-linearity, allowing deep learning**.  \n",
        "‚úÖ **Backpropagation & optimizers improve training efficiency**.  \n",
        "‚úÖ **Automatic Differentiation (AD) is essential for modern deep learning**.  \n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "eHkrhEqb6Czz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **üìå a. More Explaination**\n",
        "\n",
        "## **Q1: How does a human neuron work?**  \n",
        "In the biological brain, a **neuron** processes information in the following way:  \n",
        "\n",
        "1Ô∏è‚É£ **Dendrites** ‚Üí These are branches that **collect input signals** from other neurons.  \n",
        "2Ô∏è‚É£ **Cell Body (Soma)** ‚Üí Inside the neuron, the collected signals are **summed up**.  \n",
        "3Ô∏è‚É£ **Axon** ‚Üí If the total signal **exceeds a certain threshold**, the neuron **triggers** (fires an action potential).  \n",
        "4Ô∏è‚É£ **Synapse** ‚Üí The neuron then **sends the signal to other neurons** through its axon terminals.  \n",
        "\n",
        "üí° **Key Idea**: Neurons act as **threshold-based decision units**. If the input is strong enough, the neuron fires; otherwise, it stays inactive.  \n",
        "\n",
        "---\n",
        "\n",
        "## **Q2: What is the Heaviside function, and how was it used in early neural networks?**  \n",
        "\n",
        "The **Heaviside step function** is a simple **binary activation function** that outputs:  \n",
        "\n",
        "$$  \n",
        "H(z) =  \n",
        "\\begin{cases}  \n",
        "1, & \\text{if } z \\geq 0 \\\\  \n",
        "0, & \\text{if } z < 0  \n",
        "\\end{cases}  \n",
        "$$  \n",
        "\n",
        "In **early neural networks (Perceptrons - 1958)**, this function was used to **simulate the \"triggering\" of biological neurons**. If the weighted sum of inputs exceeded a threshold, the neuron outputted **1 (active)**; otherwise, it outputted **0 (inactive)**.  \n",
        "\n",
        "### **üîπ Why Was Heaviside Function Abandoned?**  \n",
        "‚ùå It is **not differentiable** at $z=0$, which makes training difficult using gradient-based methods.  \n",
        "‚ùå It cannot model **gradual** changes (only sharp 0/1 jumps).  \n",
        "‚úÖ **Modern NNs use smooth functions** like **Sigmoid and ReLU**, which allow gradient-based learning.  \n",
        "\n",
        "---\n",
        "\n",
        "## **Q3: What is Automatic Differentiation (AD), and what are the different techniques?**  \n",
        "\n",
        "### **üîπ What is Automatic Differentiation (AD)?**  \n",
        "AD is a technique used to **compute derivatives efficiently**. In deep learning, it is essential for **backpropagation**, as it helps update weights by calculating gradients.  \n",
        "\n",
        "There are **three main ways** to compute derivatives:  \n",
        "\n",
        "1Ô∏è‚É£ **Symbolic Differentiation** ‚Üí Uses algebraic rules to compute exact derivatives. ‚ùå **Problem:** Expressions get too large (expression explosion).  \n",
        "2Ô∏è‚É£ **Numerical Differentiation** ‚Üí Uses small step sizes ($h$) to approximate derivatives. ‚ùå **Problem:** Leads to rounding errors.  \n",
        "3Ô∏è‚É£ **Automatic Differentiation (AD)** ‚Üí Breaks down computations into elementary operations and applies the **chain rule** automatically. ‚úÖ **Most efficient** for deep learning.  \n",
        "\n",
        "---\n",
        "\n",
        "## **Q4: What are the assumptions in Backpropagation?**  \n",
        "\n",
        "Backpropagation **relies on two key assumptions** about the loss function **$L$**:  \n",
        "\n",
        "1Ô∏è‚É£ **Loss can be expressed as an average over individual training examples:**  \n",
        "   $$  \n",
        "   L = \\frac{1}{n} \\sum L_i  \n",
        "   $$  \n",
        "   - This means the total loss is simply the **average loss** of each training example.  \n",
        "   - **Why?** It makes the gradient computation **scalable** (stochastic gradient descent can update weights using mini-batches).  \n",
        "\n",
        "2Ô∏è‚É£ **Loss can be expressed as a function of network outputs:**  \n",
        "   $$  \n",
        "   L = f(y_{\\text{NN}})  \n",
        "   $$  \n",
        "   - The loss function depends **only on the predicted output** of the neural network.  \n",
        "   - **Why?** This allows us to **apply the chain rule** efficiently during backpropagation.  \n",
        "\n",
        "‚úÖ These assumptions **simplify** training and make gradient-based optimization feasible.  \n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "iClmvXdK7eX1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **üéØ 3. Physics Informed Neural Networks**  "
      ],
      "metadata": {
        "id": "dD6w7OfKGWVA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **üìå a. Physics-Informed Neural Networks (PINNs)**\n",
        "Now that we understand **how NNs work**, let‚Äôs look at **PINNs**.\n",
        "\n",
        "\n",
        "  <center>\n",
        "  <img src=\"https://i.postimg.cc/qqpyxDtM/PINNS.png\" alt=\"Image loading error. Image of Neural Network is here.\" width=\"800\" > </center>\n",
        "  <br>\n",
        "\n",
        "#### **üîπ What Makes PINNs Different?**\n",
        "- Instead of learning from **labeled data**, PINNs incorporate **differential equations directly into the loss function**.  \n",
        "- PINNs learn a function **$( y_{NN}(x))$** that satisfies both:  \n",
        "  1. **The differential equation**  \n",
        "  2. **Boundary/initial conditions**  \n",
        "\n",
        "#### **üîπ General Form of a PDE**  \n",
        "The paper presents the general form of a **Partial Differential Equation (PDE)**:  \n",
        "\n",
        "$$\n",
        "D \\left( x, y(x), \\frac{\\partial y}{\\partial x}, \\frac{\\partial^2 y}{\\partial x^2}, ... \\right) = 0\n",
        "$$\n",
        "\n",
        "where:  \n",
        "- $(x)$ = input variables  \n",
        "- $( y(x))$ = unknown function  \n",
        "- $( \\frac{\\partial y}{\\partial x})$ = first derivative  \n",
        "- $( \\frac{\\partial^2 y}{\\partial x^2})$ = second derivative  \n",
        "\n",
        "Boundary conditions are given by:  \n",
        "\n",
        "$$\n",
        "B(x, y(x)) = 0\n",
        "$$\n",
        "PINNs **approximate \\( y(x) \\)** using a neural network and ensure it satisfies both the equation and boundary conditions.  \n",
        "\n",
        "#### **üîπ How Does the PINN Loss Function Work?**\n",
        "The total loss function consists of two terms:  \n",
        "\n",
        "$$\n",
        "L(\\theta) = \\omega_D L_D(\\theta) + \\omega_B L_B(\\theta)\n",
        "$$\n",
        "\n",
        "where:  \n",
        "- **$(L_D(\\theta))$** = Loss measuring how well the **differential equation** is satisfied.  \n",
        "- **$(L_B(\\theta))$** = Loss measuring how well the **boundary/initial conditions** are satisfied.  \n",
        "- **$( \\omega_D)$ and $( \\omega_B)$** = Weights to balance both losses** (usually set to 1).  \n",
        "\n",
        "**Key Takeaways:**  \n",
        "‚úÖ PINNs use a **neural network to approximate the solution**.  \n",
        "‚úÖ The **loss function ensures the solution satisfies the PDE & boundary conditions**.  \n",
        "\n",
        "---\n",
        "\n",
        "### **üìå 4. Solving ODEs with PINNs (Example Problems)**  \n",
        "The paper presents **three types of differential equations solved using PINNs**:  \n",
        "\n",
        "#### **üîπ First-Order ODE: Exponential Decay**\n",
        "Equation:  \n",
        "$$\n",
        "y'(x) + y(x) = 0, \\quad y(0) = 1\n",
        "$$\n",
        "Exact solution:  \n",
        "$$\n",
        "y(x) = e^{-x}\n",
        "$$\n",
        "- PINN **approximates $( y(x))$ using a neural network**.  \n",
        "- The **loss function ensures** the network satisfies the equation and initial condition.  \n",
        "\n",
        "#### **üîπ Second-Order ODE: Harmonic Oscillator**\n",
        "Equation:  \n",
        "$$\n",
        "y''(x) + y(x) = 0, \\quad y(0) = 1, \\quad y'(0) = 0\n",
        "$$\n",
        "Exact solution:  \n",
        "$$\n",
        "y(x) = \\cos(x)\n",
        "$$\n",
        "- Models **simple harmonic motion** (e.g., pendulums, circuits).  \n",
        "- PINN trains to satisfy both the equation and initial conditions.  \n",
        "\n",
        "#### **üîπ Second-Order Nonlinear ODE: Soliton Solution**\n",
        "Equation:  \n",
        "$$\n",
        "y''(x) - y(x) - 3y^2(x) = 0\n",
        "$$\n",
        "Exact solution:  \n",
        "$$\n",
        "y(x) = -\\frac{1}{2} \\text{sech}^2 \\left( \\frac{x}{2} \\right)\n",
        "$$\n",
        "- Used in **fluid dynamics, fiber optics, quantum mechanics**.  \n",
        "- Requires more **training time & a lower learning rate**.  \n",
        "\n",
        "**Key Takeaways:**  \n",
        "‚úÖ PINNs successfully approximate solutions for **different types of equations**.  \n",
        "‚úÖ They work well for **both linear and nonlinear equations**.  \n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "JJrqSerSGWRU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## **üìå b. More Explaination**  \n",
        "\n",
        "### **1Ô∏è‚É£ Understanding the General PDE Form**  \n",
        "\n",
        "The general PDE given in the paper is:  \n",
        "\n",
        "$$\n",
        "D(\\mathbf{x}, y(\\mathbf{x}), \\frac{\\partial y}{\\partial x_1}, ..., \\frac{\\partial y}{\\partial x_d}, \\frac{\\partial^2 y}{\\partial x_1^2}, ..., \\frac{\\partial^2 y}{\\partial x_1 \\partial x_d}, ...) = 0\n",
        "$$  \n",
        "\n",
        "#### **üîπ What is $D$?**  \n",
        "- $D$ represents the **differential operator**, which contains:\n",
        "  - The function itself: $ y(\\mathbf{x}) $\n",
        "  - First-order derivatives: $ \\frac{\\partial y}{\\partial x_1}, ..., \\frac{\\partial y}{\\partial x_d} $\n",
        "  - Second-order derivatives: $ \\frac{\\partial^2 y}{\\partial x_1^2}, ..., \\frac{\\partial^2 y}{\\partial x_1 \\partial x_d} $\n",
        "- This equation **must hold at every point in the domain**.  \n",
        "\n",
        "For example, in a **heat equation**:  \n",
        "\n",
        "$$\n",
        "\\frac{\\partial u}{\\partial t} - \\alpha \\frac{\\partial^2 u}{\\partial x^2} = 0\n",
        "$$  \n",
        "\n",
        "Here, $D$ is:  \n",
        "\n",
        "$$\n",
        "D = \\frac{\\partial u}{\\partial t} - \\alpha \\frac{\\partial^2 u}{\\partial x^2}\n",
        "$$  \n",
        "\n",
        "#### **üîπ What is $\\mathbf{x}$?**  \n",
        "- The notation $ \\mathbf{x} = (x_1, ..., x_d) $ means a **$d$-dimensional input space**:\n",
        "  - If $ d = 1 $, then $ x_1 $ could be time $ t $ or position $ x $.\n",
        "  - If $ d = 2 $, then $ (x_1, x_2) $ could be spatial coordinates $ (x, y) $.\n",
        "  - If $ d = 3 $, then $ (x_1, x_2, x_3) $ could be $ (x, y, z) $.  \n",
        "\n",
        "#### **üîπ What are the Partial Derivatives?**  \n",
        "- First-order derivatives:  \n",
        "  $$\n",
        "  \\frac{\\partial y}{\\partial x_1}, ..., \\frac{\\partial y}{\\partial x_d}\n",
        "  $$  \n",
        "  These measure the rate of change of $ y(x) $ with respect to each variable.  \n",
        "- Second-order derivatives:  \n",
        "  $$\n",
        "  \\frac{\\partial^2 y}{\\partial x_1^2}, ..., \\frac{\\partial^2 y}{\\partial x_1 \\partial x_d}\n",
        "  $$  \n",
        "  These measure how the rate of change itself is changing.  \n",
        "\n",
        "For example:  \n",
        "- $ \\frac{\\partial^2 y}{\\partial x_1^2} $ measures acceleration in the $ x_1 $ direction.  \n",
        "- $ \\frac{\\partial^2 y}{\\partial x_1 \\partial x_d} $ is a **mixed derivative**, showing how $ y(x) $ changes in response to both $ x_1 $ and $ x_d $.  \n",
        "\n",
        "---\n",
        "\n",
        "### **2Ô∏è‚É£ Understanding the Boundary Conditions**  \n",
        "\n",
        "The equation:  \n",
        "\n",
        "$$\n",
        "B(\\mathbf{x}, y(\\mathbf{x})) = 0, \\quad \\mathbf{x} \\in \\partial\\Omega\n",
        "$$  \n",
        "\n",
        "represents the **boundary conditions**, which tell us the values of $ y(x) $ at the domain edges.  \n",
        "\n",
        "#### **üîπ Example:**  \n",
        "For a heat equation on a rod, we could have:  \n",
        "\n",
        "$$\n",
        "u(0, t) = 0, \\quad u(L, t) = 100\n",
        "$$  \n",
        "\n",
        "These conditions **must be satisfied** in addition to the PDE.  \n",
        "\n",
        "---\n",
        "\n",
        "### **3Ô∏è‚É£ Understanding the Loss Functions in PINNs**  \n",
        "\n",
        "The total loss function has two parts:  \n",
        "\n",
        "$$\n",
        "L(\\theta) = \\omega_D L_D + \\omega_B L_B\n",
        "$$  \n",
        "\n",
        "#### **üîπ PDE Loss: $ L_D $**  \n",
        "The PINN must **minimize the PDE residual** at selected points:  \n",
        "\n",
        "$$\n",
        "L_D(\\theta, \\Gamma_D) = \\frac{1}{N_D} \\sum_{\\mathbf{x} \\in \\Gamma_D} D(\\mathbf{x}, y_{NN}(\\mathbf{x}), \\frac{\\partial y_{NN}}{\\partial x_1}, ..., \\frac{\\partial^2 y_{NN}}{\\partial x_1^2}, ...)^2\n",
        "$$  \n",
        "\n",
        "- This is the **mean squared error (MSE)** of the PDE.  \n",
        "- The neural network $ y_{NN} $ must satisfy $ D = 0 $.  \n",
        "\n",
        "#### **üîπ Boundary Loss: $ L_B $**  \n",
        "The PINN must also **enforce boundary conditions**:  \n",
        "\n",
        "$$\n",
        "L_B(\\theta, \\Gamma_B) = \\frac{1}{N_B} \\sum_{\\mathbf{x} \\in \\Gamma_B} |B(y_{NN}, \\mathbf{x})|^2\n",
        "$$  \n",
        "\n",
        "- This ensures the network obeys the boundary conditions.  \n",
        "\n",
        "#### **üîπ Total Loss Function:**  \n",
        "$$\n",
        "L(\\theta) = \\omega_D L_D + \\omega_B L_B\n",
        "$$  \n",
        "where $ \\omega_D $ and $ \\omega_B $ are weights balancing the PDE and boundary losses.  \n",
        "\n",
        "---\n",
        "\n",
        "### **4Ô∏è‚É£ Numerical Example**  \n",
        "\n",
        "Solve the **1D PDE**:  \n",
        "\n",
        "$$\n",
        "\\frac{d^2 y}{dx^2} = -\\sin(x), \\quad 0 \\leq x \\leq \\pi\n",
        "$$  \n",
        "\n",
        "with boundary conditions:  \n",
        "\n",
        "$$\n",
        "y(0) = 0, \\quad y(\\pi) = 0\n",
        "$$  \n",
        "\n",
        "#### **üîπ Step 1: Express in PINN Form**  \n",
        "- Neural network approximation: $ y_{NN}(x) $  \n",
        "- First derivative: $ \\frac{d y_{NN}}{dx} $  \n",
        "- Second derivative: $ \\frac{d^2 y_{NN}}{dx^2} $  \n",
        "\n",
        "#### **üîπ Step 2: Define Loss Functions**  \n",
        "$$\n",
        "L_D = \\frac{1}{N_D} \\sum_{x \\in \\Gamma_D} \\left( \\frac{d^2 y_{NN}}{dx^2} + \\sin(x) \\right)^2\n",
        "$$  \n",
        "\n",
        "$$\n",
        "L_B = \\frac{1}{N_B} \\sum_{x \\in \\Gamma_B} \\left( y_{NN}(0)^2 + y_{NN}(\\pi)^2 \\right)\n",
        "$$  \n",
        "\n",
        "#### **üîπ Step 3: Train Neural Network**  \n",
        "1. **Use a simple feedforward network** with input $ x $ and output $ y_{NN}(x) $.  \n",
        "2. **Compute derivatives using automatic differentiation** (PyTorch).  \n",
        "3. **Minimize $ L = L_D + L_B $ using gradient descent**.  \n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "$$\n",
        "\\frac{d^2 y}{dx^2} - 3 \\frac{dy}{dx} + 2y = 0\n",
        "$$\n",
        "\n",
        "This is a **linear, second-order ODE**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 2: Express in the General Operator Form**  \n",
        "The general operator notation is:\n",
        "\n",
        "$$\n",
        "D\\left(x, y, \\frac{\\partial y}{\\partial x}, \\frac{\\partial^2 y}{\\partial x^2}, \\dots \\right) = 0\n",
        "$$\n",
        "\n",
        "For our equation, we define the operator $( D(y))$ as:\n",
        "\n",
        "$$\n",
        "D(y) = \\frac{d^2 y}{dx^2} - 3 \\frac{dy}{dx} + 2y\n",
        "$$\n",
        "\n",
        "Thus, in general operator form:\n",
        "\n",
        "$$\n",
        "D\\left(x, y, \\frac{\\partial y}{\\partial x}, \\frac{\\partial^2 y}{\\partial x^2} \\right) = \\frac{\\partial^2 y}{\\partial x^2} - 3 \\frac{\\partial y}{\\partial x} + 2y = 0\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 3: Apply the Loss Function**\n",
        "From our loss function formula:\n",
        "\n",
        "$$\n",
        "L_D(\\theta, \\Gamma_D) = \\frac{1}{N_D} \\sum_{x \\in \\Gamma_D} \\left| D \\left(x, y_{NN}, \\frac{\\partial y_{NN}}{\\partial x}, \\frac{\\partial^2 y_{NN}}{\\partial x^2} \\right) \\right|^2\n",
        "$$\n",
        "\n",
        "Substituting our specific \\( D(y) \\):\n",
        "\n",
        "$$\n",
        "L_D(\\theta) = \\frac{1}{N} \\sum_{x \\in \\Gamma_D} \\left| \\frac{\\partial^2 y_{NN}}{\\partial x^2} - 3 \\frac{\\partial y_{NN}}{\\partial x} + 2 y_{NN} \\right|^2\n",
        "$$\n",
        "\n",
        "This means:\n",
        "1. Compute the **first derivative** $( \\frac{\\partial y_{NN}}{\\partial x})$.\n",
        "2. Compute the **second derivative** $( \\frac{\\partial^2 y_{NN}}{\\partial x^2})$ using **automatic differentiation**.\n",
        "3. Evaluate $( \\left(\\frac{\\partial^2 y_{NN}}{\\partial x^2} - 3 \\frac{\\partial y_{NN}}{\\partial x} + 2 y_{NN} \\right) )$ at multiple points.\n",
        "4. Square the residual and take the average over all points.\n",
        "\n",
        "---\n",
        "\n",
        "### **5Ô∏è‚É£ Summary**  \n",
        "‚úÖ $ D $ is the **PDE differential operator**.  \n",
        "‚úÖ It contains **function values, first derivatives, and second derivatives**.  \n",
        "‚úÖ PINNs minimize **both PDE loss ($L_D$) and boundary loss ($L_B$)**.  \n",
        "‚úÖ We approximate $ y(x) $ using a neural network $ y_{NN}(x) $.  \n",
        "‚úÖ Training ensures that $ y_{NN} $ satisfies **both the PDE and boundary conditions**.  \n"
      ],
      "metadata": {
        "id": "XZP6eBFNEQiy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8ZWQLrCxDN3V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}